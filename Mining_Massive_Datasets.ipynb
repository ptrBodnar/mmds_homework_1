{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sq8U3BtmhtRx"
   },
   "source": [
    "\n",
    "# **Running Pyspark in Colab**\n",
    "\n",
    "To run spark in Colab, we need to first install all the dependencies in Colab environment i.e. Apache Spark 2.3.2 with hadoop 2.7, Java 8 and Findspark to locate the spark in the system. The tools installation can be carried out inside the Jupyter Notebook of the Colab. One important note is that if you are new in Spark, it is better to avoid Spark 2.4.0 version since some people have already complained about its compatibility issue with python. \n",
    "Follow the steps to install the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lh5NCoc8fsSO"
   },
   "outputs": [],
   "source": [
    "# !wget -q https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
    "# !tar xf spark-3.0.1-bin-hadoop2.7.tgz\n",
    "# !pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILheUROOhprv"
   },
   "source": [
    "Now that you installed Spark and Java in Colab, it is time to set the environment path which enables you to run Pyspark in your Colab environment. Set the location of Java and Spark by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T18:33:16.505254Z",
     "start_time": "2020-11-03T18:33:16.501247Z"
    },
    "id": "v1b8k_OVf2QF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"../spark-3.0.1-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwrqMk3HiMiE"
   },
   "source": [
    "Run a local spark session to test your installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T18:33:23.911480Z",
     "start_time": "2020-11-03T18:33:18.483819Z"
    },
    "id": "9_Uz1NL4gHFx"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T18:42:50.192864Z",
     "start_time": "2020-11-03T18:42:45.809229Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "vERDoZz3TXx2",
    "outputId": "da327a51-5b0b-4f7b-818d-dc731d964ef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-03 20:42:45--  http://data.insideairbnb.com/spain/catalonia/barcelona/2020-09-12/visualisations/listings.csv\n",
      "Resolving data.insideairbnb.com (data.insideairbnb.com)... 52.216.77.19\n",
      "Connecting to data.insideairbnb.com (data.insideairbnb.com)|52.216.77.19|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3137507 (3,0M) [application/csv]\n",
      "Saving to: ‘listings.csv’\n",
      "\n",
      "listings.csv        100%[===================>]   2,99M   836KB/s    in 3,7s    \n",
      "\n",
      "2020-11-03 20:42:50 (836 KB/s) - ‘listings.csv’ saved [3137507/3137507]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ! wget http://data.insideairbnb.com/the-netherlands/north-holland/amsterdam/2020-08-18/data/listings.csv\n",
    "# ! wget http://data.insideairbnb.com/the-netherlands/north-holland/amsterdam/2020-08-18/data/reviews.csv\n",
    "# ! wget http://data.insideairbnb.com/spain/catalonia/barcelona/2020-09-12/visualisations/listings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T18:33:34.770876Z",
     "start_time": "2020-11-03T18:33:27.387255Z"
    },
    "id": "zxwi2KauVTTD"
   },
   "outputs": [],
   "source": [
    "listings = spark.read.csv('listings.csv',inferSchema=True, header =True)\n",
    "# reviews = spark.read.csv('reviews.csv',inferSchema=True, header =True)\n",
    "\n",
    "train = spark.read.parquet('train_test_data/train.pkt')\n",
    "test = spark.read.parquet('train_test_data/test.pkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listings_df_pd = pd.read_csv(\"listings.csv\")\n",
    "# listings_df_pd.to_parquet(\"listing.pkt\")\n",
    "# listings = spark.read.parquet(\"listing.pkt\")\n",
    "# listings_not_null = listings[listings.name.isNotNull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T17:42:22.592772Z",
     "start_time": "2020-11-03T17:42:22.528366Z"
    },
    "id": "13ZurAGz4nwv"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"name\", outputCol=\"CleanTokens\")\n",
    "stopwordsremover = StopWordsRemover(inputCol=\"CleanTokens\", outputCol=\"CleanTokensStopRemoved\")\n",
    "hashingTF = HashingTF(inputCol=\"CleanTokensStopRemoved\", outputCol=\"VectorSpace\")\n",
    "idf = IDF(inputCol=\"VectorSpace\", outputCol=\"VectorSpaceIDF\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T17:42:23.365132Z",
     "start_time": "2020-11-03T17:42:22.787562Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, stopwordsremover, hashingTF, idf])\n",
    "pipelineModel = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T17:42:23.456766Z",
     "start_time": "2020-11-03T17:42:23.368339Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pipe = pipelineModel.transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T17:42:27.624371Z",
     "start_time": "2020-11-03T17:42:27.504729Z"
    }
   },
   "outputs": [],
   "source": [
    "brp = BucketedRandomProjectionLSH(inputCol=\"VectorSpaceIDF\", outputCol=\"hashes\", bucketLength=2.0,\n",
    "                                  numHashTables=3)\n",
    "model = brp.fit(df_pipe)\n",
    "df_pipe = model.transform(df_pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = df_pipe.sample(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T18:29:44.107858Z",
     "start_time": "2020-11-03T18:29:43.751340Z"
    }
   },
   "outputs": [],
   "source": [
    "pipelineModelTest = pipeline.fit(test)\n",
    "test_prepared = pipelineModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T18:29:45.202495Z",
     "start_time": "2020-11-03T18:29:44.109289Z"
    }
   },
   "outputs": [],
   "source": [
    "test_pd = test_prepared.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T18:29:45.206990Z",
     "start_time": "2020-11-03T18:29:45.204332Z"
    }
   },
   "outputs": [],
   "source": [
    "looking_for = test_pd.VectorSpaceIDF.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T17:45:49.533796Z",
     "start_time": "2020-11-03T17:45:49.528178Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_neighbours(data, value, number, colName):\n",
    "    result = model.approxNearestNeighbors(data, value, number, distCol=colName)\n",
    "  \n",
    "    return result.select(\"id\").toPandas()['id'][1:].to_list()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T18:08:43.652302Z",
     "start_time": "2020-11-03T17:47:00.299608Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-140-117033a312df>:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for key in tqdm.tqdm_notebook(looking_for):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871266ec9a87451ebb8ca65f9425c485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3784.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "out = []\n",
    "for key in tqdm.notebook.tqdm(looking_for):\n",
    "    neigh = find_neighbours(test_prepared, key, 6, 'hashes')\n",
    "    out.append(neigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T18:08:43.658854Z",
     "start_time": "2020-11-03T18:08:43.656265Z"
    }
   },
   "outputs": [],
   "source": [
    "# predict = [find_neighbours(test_prepared, key, 6, 'hashes') for key in looking_for]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T18:15:03.997679Z",
     "start_time": "2020-11-03T18:15:03.994015Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T18:29:51.487105Z",
     "start_time": "2020-11-03T18:29:51.481546Z"
    }
   },
   "outputs": [],
   "source": [
    "def compare_lists(a,b):\n",
    "    results = []\n",
    "    for i, _ in enumerate(a):\n",
    "        intersection = set(a[i]).intersection(b[i])\n",
    "        results.append(len(intersection))\n",
    "        \n",
    "    return np.mean(results), results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T18:29:52.279773Z",
     "start_time": "2020-11-03T18:29:52.244088Z"
    }
   },
   "outputs": [],
   "source": [
    "score, all_res = compare_lists(out, test_pd['ground_truth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T18:29:56.122746Z",
     "start_time": "2020-11-03T18:29:56.117202Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.714323467230444"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T18:17:16.738134Z",
     "start_time": "2020-11-03T18:17:16.729899Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.69027484143763"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3129/3784 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_bucket_length = [2, 6, 10]\n",
    "grid_num_hash_tables = [3, 7, 10]\n",
    "\n",
    "def find_neighbours(model_, data, value, number, colName):\n",
    "    result = model_.approxNearestNeighbors(data, value, number, distCol=colName)\n",
    "  \n",
    "    return result.select(\"id\").toPandas()['id'][1:].to_list()\n",
    "\n",
    "def compare_lists(a,b):\n",
    "    results = []\n",
    "    for i, _ in enumerate(a):\n",
    "        intersection = set(a[i]).intersection(b[i])\n",
    "        results.append(len(intersection))\n",
    "        \n",
    "#     return np.mean(results), results\n",
    "    return np.mean(results), np.sum(np.array(results) > 1)/len(b), np.sum(np.array(results) == 5)/len(b), results\n",
    "\n",
    "\n",
    "def grid_search_lsh(train_data, test_data, grid_bucket_length, grid_num_hash_tables, targets, limit=100):\n",
    "    results = []  \n",
    "    for bucket_length in grid_bucket_length:\n",
    "        for n_hash_table in grid_num_hash_tables: \n",
    "            brp = BucketedRandomProjectionLSH(inputCol=\"VectorSpaceIDF\", \n",
    "                                              outputCol=\"hashes\", \n",
    "                                              bucketLength=bucket_length,\n",
    "                                              numHashTables=n_hash_table)\n",
    "            # fit train\n",
    "            model = brp.fit(train_data)\n",
    "            print(f'Models params: \\nbucket: {model.getBucketLength()}, n_ht: {model.getNumHashTables()}')\n",
    "            \n",
    "            df_pipe = model.transform(train_data)\n",
    "            looking_for = targets.VectorSpaceIDF.to_list()\n",
    "            print(f'Calculating LSH for bucket_length={bucket_length} and numHashTables = {n_hash_table}')\n",
    "            test = test_data.limit(limit)\n",
    "            targ = looking_for[:limit]\n",
    "            prediction = []\n",
    "            \n",
    "            for key in tqdm.tqdm_notebook(targ):\n",
    "                neigh = find_neighbours(model, test, key, 6, 'hashes')\n",
    "                prediction.append(neigh)\n",
    "\n",
    "            score_a, score_b, score_c, num_neighb = compare_lists(prediction, targets['ground_truth'])\n",
    "            print(f'Total score: {score_a}, {score_b}, {score_c}\\n {num_neighb[:20]}')\n",
    "            results.append([bucket_length, n_hash_table, score_a, score_b, score_c, num_neighb])\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = grid_search_lsh(df_pipe, test_prepared, grid_bucket_length, grid_num_hash_tables, test_pd)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Mining Massive Datasets",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
