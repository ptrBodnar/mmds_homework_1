{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sq8U3BtmhtRx"
   },
   "source": [
    "\n",
    "# **Running Pyspark in Colab**\n",
    "\n",
    "To run spark in Colab, we need to first install all the dependencies in Colab environment i.e. Apache Spark 2.3.2 with hadoop 2.7, Java 8 and Findspark to locate the spark in the system. The tools installation can be carried out inside the Jupyter Notebook of the Colab. One important note is that if you are new in Spark, it is better to avoid Spark 2.4.0 version since some people have already complained about its compatibility issue with python. \n",
    "Follow the steps to install the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lh5NCoc8fsSO"
   },
   "outputs": [],
   "source": [
    "# !wget -q https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
    "# !tar xf spark-3.0.1-bin-hadoop2.7.tgz\n",
    "# !pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILheUROOhprv"
   },
   "source": [
    "Now that you installed Spark and Java in Colab, it is time to set the environment path which enables you to run Pyspark in your Colab environment. Set the location of Java and Spark by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T19:02:27.236852Z",
     "start_time": "2020-11-03T19:02:27.232456Z"
    },
    "id": "v1b8k_OVf2QF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"../spark-3.0.1-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwrqMk3HiMiE"
   },
   "source": [
    "Run a local spark session to test your installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T19:02:33.557796Z",
     "start_time": "2020-11-03T19:02:27.988493Z"
    },
    "id": "9_Uz1NL4gHFx"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T18:42:50.192864Z",
     "start_time": "2020-11-03T18:42:45.809229Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "vERDoZz3TXx2",
    "outputId": "da327a51-5b0b-4f7b-818d-dc731d964ef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-03 20:42:45--  http://data.insideairbnb.com/spain/catalonia/barcelona/2020-09-12/visualisations/listings.csv\n",
      "Resolving data.insideairbnb.com (data.insideairbnb.com)... 52.216.77.19\n",
      "Connecting to data.insideairbnb.com (data.insideairbnb.com)|52.216.77.19|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3137507 (3,0M) [application/csv]\n",
      "Saving to: ‘listings.csv’\n",
      "\n",
      "listings.csv        100%[===================>]   2,99M   836KB/s    in 3,7s    \n",
      "\n",
      "2020-11-03 20:42:50 (836 KB/s) - ‘listings.csv’ saved [3137507/3137507]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ! wget http://data.insideairbnb.com/the-netherlands/north-holland/amsterdam/2020-08-18/data/listings.csv\n",
    "# ! wget http://data.insideairbnb.com/the-netherlands/north-holland/amsterdam/2020-08-18/data/reviews.csv\n",
    "# ! wget http://data.insideairbnb.com/spain/catalonia/barcelona/2020-09-12/visualisations/listings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T19:02:33.768238Z",
     "start_time": "2020-11-03T19:02:33.560618Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T19:26:03.153074Z",
     "start_time": "2020-11-03T19:26:02.700600Z"
    },
    "id": "zxwi2KauVTTD"
   },
   "outputs": [],
   "source": [
    "listings = spark.read.csv('listings.csv',inferSchema=True, header =True)\n",
    "\n",
    "train = spark.read.parquet('train_test_data/train.pkt')\n",
    "test = spark.read.parquet('train_test_data/test.pkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T19:26:03.160093Z",
     "start_time": "2020-11-03T19:26:03.156676Z"
    }
   },
   "outputs": [],
   "source": [
    "# listings_df_pd = pd.read_csv(\"listings.csv\")\n",
    "# listings_df_pd.to_parquet(\"listing.pkt\")\n",
    "# listings = spark.read.parquet(\"listing.pkt\")\n",
    "# listings_not_null = listings[listings.name.isNotNull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T19:26:03.217287Z",
     "start_time": "2020-11-03T19:26:03.163182Z"
    },
    "id": "13ZurAGz4nwv"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"name\", outputCol=\"CleanTokens\")\n",
    "stopwordsremover = StopWordsRemover(inputCol=\"CleanTokens\", outputCol=\"CleanTokensStopRemoved\")\n",
    "hashingTF = HashingTF(inputCol=\"CleanTokensStopRemoved\", outputCol=\"VectorSpace\")\n",
    "idf = IDF(inputCol=\"VectorSpace\", outputCol=\"VectorSpaceIDF\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T19:26:03.618386Z",
     "start_time": "2020-11-03T19:26:03.265424Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, stopwordsremover, hashingTF, idf])\n",
    "pipelineModel = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T19:26:03.703399Z",
     "start_time": "2020-11-03T19:26:03.622553Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pipe = pipelineModel.transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T19:26:04.139758Z",
     "start_time": "2020-11-03T19:26:03.809053Z"
    }
   },
   "outputs": [],
   "source": [
    "brp = BucketedRandomProjectionLSH(inputCol=\"VectorSpaceIDF\", outputCol=\"hashes\", bucketLength=20,\n",
    "                                  numHashTables=10)\n",
    "model = brp.fit(df_pipe)\n",
    "df_pipe = model.transform(df_pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T19:26:04.871315Z",
     "start_time": "2020-11-03T19:26:04.561575Z"
    }
   },
   "outputs": [],
   "source": [
    "pipelineModelTest = pipeline.fit(test)\n",
    "test_prepared = pipelineModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T19:26:05.908675Z",
     "start_time": "2020-11-03T19:26:04.875565Z"
    }
   },
   "outputs": [],
   "source": [
    "test_pd = test_prepared.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T19:26:05.918495Z",
     "start_time": "2020-11-03T19:26:05.911958Z"
    }
   },
   "outputs": [],
   "source": [
    "looking_for = test_pd.VectorSpaceIDF.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T19:26:05.926479Z",
     "start_time": "2020-11-03T19:26:05.922586Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_neighbours(data, value, number, colName):\n",
    "    result = model.approxNearestNeighbors(data, value, number, distCol=colName)\n",
    "  \n",
    "    return result.select(\"id\").toPandas()['id'][1:].to_list()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T19:53:09.460474Z",
     "start_time": "2020-11-03T19:26:05.928689Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-68-117033a312df>:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for key in tqdm.tqdm_notebook(looking_for):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d62c6cc8e9474aab7fc73697cdfac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4065.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "out = []\n",
    "for key in tqdm.tqdm_notebook(looking_for):\n",
    "    neigh = find_neighbours(test_prepared, key, 6, 'hashes')\n",
    "    out.append(neigh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's slow anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T19:53:09.466196Z",
     "start_time": "2020-11-03T19:53:09.462816Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T19:56:59.912909Z",
     "start_time": "2020-11-03T19:56:59.906550Z"
    }
   },
   "outputs": [],
   "source": [
    "def compare_lists(a,b):\n",
    "    results = []\n",
    "    for i, _ in enumerate(a):\n",
    "        intersection = set(a[i]).intersection(b[i])\n",
    "        results.append(len(intersection))\n",
    "        \n",
    "    return np.mean(results), np.sum(np.array(results) > 1)/len(b), np.sum(np.array(results) == 5)/len(b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T19:57:00.174893Z",
     "start_time": "2020-11-03T19:57:00.137465Z"
    }
   },
   "outputs": [],
   "source": [
    "score_a, score_b, score_c = compare_lists(out, test_pd['ground_truth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T19:56:53.802489Z",
     "start_time": "2020-11-03T19:56:53.797218Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.7424354243542435, 0.5682656826568265, 0.07060270602706027)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_a, score_b, score_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T19:10:00.787612Z",
     "start_time": "2020-11-03T19:10:00.777877Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177           Beautiful Apartment Sagrada Familia\n",
       "206           Gran Vía Apartment Catalunya square\n",
       "285            Beautiful Double Room With Balcony\n",
       "327                                Cozy Apartment\n",
       "331    Beautiful Double Room with private terrace\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pd.loc[test_pd.id.isin(out[5])].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T19:10:47.778300Z",
     "start_time": "2020-11-03T19:10:47.773667Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10894127, 10874782, 543342, 43482809, 10127485]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pd['ground_truth'][5]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Mining Massive Datasets",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
