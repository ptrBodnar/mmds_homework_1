{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sq8U3BtmhtRx"
   },
   "source": [
    "\n",
    "#  **Running Pyspark in Colab**\n",
    "\n",
    "To run spark in Colab, we need to first install all the dependencies in Colab environment i.e. Apache Spark 2.3.2 with hadoop 2.7, Java 8 and Findspark to locate the spark in the system. The tools installation can be carried out inside the Jupyter Notebook of the Colab. One important note is that if you are new in Spark, it is better to avoid Spark 2.4.0 version since some people have already complained about its compatibility issue with python. \n",
    "Follow the steps to install the dependencies:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gdgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lh5NCoc8fsSO"
   },
   "outputs": [],
   "source": [
    "# !wget -q https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
    "# !tar xf spark-3.0.1-bin-hadoop2.7.tgz\n",
    "# !pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILheUROOhprv"
   },
   "source": [
    "Now that you installed Spark and Java in Colab, it is time to set the environment path which enables you to run Pyspark in your Colab environment. Set the location of Java and Spark by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T21:32:25.083933Z",
     "start_time": "2020-11-03T21:32:25.079854Z"
    },
    "id": "v1b8k_OVf2QF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"../spark-3.0.1-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwrqMk3HiMiE"
   },
   "source": [
    "Run a local spark session to test your installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T21:32:25.677149Z",
     "start_time": "2020-11-03T21:32:25.671448Z"
    },
    "id": "9_Uz1NL4gHFx"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import requests\n",
    "\n",
    "knn = NearestNeighbors()\n",
    "td_idf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Downloaded data and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T18:42:50.192864Z",
     "start_time": "2020-11-03T18:42:45.809229Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "hidden": true,
    "id": "vERDoZz3TXx2",
    "outputId": "da327a51-5b0b-4f7b-818d-dc731d964ef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-03 20:42:45--  http://data.insideairbnb.com/spain/catalonia/barcelona/2020-09-12/visualisations/listings.csv\n",
      "Resolving data.insideairbnb.com (data.insideairbnb.com)... 52.216.77.19\n",
      "Connecting to data.insideairbnb.com (data.insideairbnb.com)|52.216.77.19|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3137507 (3,0M) [application/csv]\n",
      "Saving to: ‘listings.csv’\n",
      "\n",
      "listings.csv        100%[===================>]   2,99M   836KB/s    in 3,7s    \n",
      "\n",
      "2020-11-03 20:42:50 (836 KB/s) - ‘listings.csv’ saved [3137507/3137507]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ! wget http://data.insideairbnb.com/spain/catalonia/barcelona/2020-09-12/visualisations/listings.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Barcelon data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "! mkdir train_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "listings_df_pd = pd.read_csv(\"listings.csv\")\n",
    "listings_df_pd_filtered = listings_df_pd[~listings_df_pd.name.isnull()].reset_index(drop=True)\n",
    "\n",
    "train, test = train_test_split(listings_df_pd_filtered, test_size=0.2, random_state=42)\n",
    "it_idf_names = td_idf_vectorizer.fit_transform(test.name)\n",
    "\n",
    "knn.fit(X=it_idf_names)\n",
    "\n",
    "five_closest = [list(knn.kneighbors(d,6)[1][0][1:] )for d in it_idf_names]\n",
    "test['ground_truth'] = [test.iloc[d].id.to_list() for d in five_closest ]\n",
    "\n",
    "test.to_parquet(\"train_test_data/test.pkt\")\n",
    "train.to_parquet(\"train_test_data/train.pkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "WikiData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "api_wiki = \"https://wikimedia.org/api/rest_v1/metrics/pageviews/top/uk.wikisource/all-access/2019/04/all-days\"\n",
    "wiki = pd.DataFrame(requests.get(api_wiki).json()['items'][0]['articles'])\n",
    "\n",
    "wiki['id'] = wiki.index\n",
    "wiki_filtered = wiki[~wiki.Page.isnull()].reset_index(drop=True)\n",
    "\n",
    "train, test = train_test_split(wiki_filtered, test_size=0.2, random_state=42)\n",
    "it_idf_names = td_idf_vectorizer.fit_transform(test.Page)\n",
    "\n",
    "knn.fit(X=it_idf_names)\n",
    "\n",
    "five_closest = [list(knn.kneighbors(d,6)[1][0][1:] )for d in it_idf_names]\n",
    "test['ground_truth'] = [test.iloc[d].id.to_list() for d in five_closest ]\n",
    "\n",
    "test.to_parquet(\"train_test_data/wiki_test.pkt\")\n",
    "train.to_parquet(\"train_test_data/wiki_train.pkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "listings_df_pd = pd.read_csv(\"listings.csv\")\n",
    "listings_df_pd_filtered = listings_df_pd[~listings_df_pd.name.isnull()].reset_index(drop=True)\n",
    "\n",
    "train, test = train_test_split(listings_df_pd_filtered, test_size=0.2, random_state=42)\n",
    "it_idf_names = td_idf_vectorizer.fit_transform(test.name)\n",
    "\n",
    "knn.fit(X=it_idf_names)\n",
    "\n",
    "five_closest = [list(knn.kneighbors(d,6)[1][0][1:] )for d in it_idf_names]\n",
    "test['ground_truth'] = [test.iloc[d].id.to_list() for d in five_closest ]\n",
    "\n",
    "test.to_parquet(\"train_test_data/test.pkt\")\n",
    "train.to_parquet(\"train_test_data/train.pkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Barcelona experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T21:32:27.584949Z",
     "start_time": "2020-11-03T21:32:27.580417Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T22:35:41.491611Z",
     "start_time": "2020-11-03T22:35:41.062126Z"
    },
    "id": "zxwi2KauVTTD"
   },
   "outputs": [],
   "source": [
    "# listings = spark.read.csv('listings.csv',inferSchema=True, header =True)\n",
    "\n",
    "train = spark.read.parquet('train_test_data/train.pkt')\n",
    "test = spark.read.parquet('train_test_data/test.pkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T22:35:42.079587Z",
     "start_time": "2020-11-03T22:35:42.030483Z"
    },
    "id": "13ZurAGz4nwv"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"name\", outputCol=\"CleanTokens\")\n",
    "stopwordsremover = StopWordsRemover(inputCol=\"CleanTokens\", outputCol=\"CleanTokensStopRemoved\")\n",
    "hashingTF = HashingTF(inputCol=\"CleanTokensStopRemoved\", outputCol=\"VectorSpace\", numFeatures=50)\n",
    "idf = IDF(inputCol=\"VectorSpace\", outputCol=\"VectorSpaceIDF\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T22:35:42.608866Z",
     "start_time": "2020-11-03T22:35:42.225291Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, stopwordsremover, hashingTF, idf])\n",
    "\n",
    "pipelineModel = pipeline.fit(train)\n",
    "pipelineModelTest = pipeline.fit(test)\n",
    "\n",
    "test_prepared = pipelineModel.transform(test)\n",
    "df_pipe = pipelineModel.transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T22:35:45.479648Z",
     "start_time": "2020-11-03T22:35:44.248893Z"
    }
   },
   "outputs": [],
   "source": [
    "test_pd = test_prepared.toPandas()\n",
    "looking_for = test_pd.VectorSpaceIDF.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T22:46:44.992587Z",
     "start_time": "2020-11-03T22:46:44.977755Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_neighbours(model_, data, value, number, colName):\n",
    "    result = model_.approxNearestNeighbors(data, value, number, distCol=colName)\n",
    "    \n",
    "    return result.select(\"id\").toPandas()['id'][1:].to_list()\n",
    "\n",
    "def compare_lists(a,b):\n",
    "    results = []\n",
    "    for i, _ in enumerate(a):\n",
    "        intersection = set(a[i]).intersection(b[i])\n",
    "        results.append(len(intersection))\n",
    "        \n",
    "#     return np.mean(results), results\n",
    "    return np.mean(results), np.sum(np.array(results) > 1)/len(b), np.sum(np.array(results) == 5)/len(b), results\n",
    "\n",
    "\n",
    "def grid_search_lsh(train_data, test_data, grid_bucket_length, grid_num_hash_tables, targets, limit=300):\n",
    "    results = []  \n",
    "    for bucket_length in grid_bucket_length:\n",
    "        for n_hash_table in grid_num_hash_tables: \n",
    "            brp = BucketedRandomProjectionLSH(inputCol=\"VectorSpaceIDF\", \n",
    "                                              outputCol=\"hashes\", \n",
    "                                              bucketLength=bucket_length,\n",
    "                                              numHashTables=n_hash_table)\n",
    "            # fit train\n",
    "            model = brp.fit(train_data)\n",
    "            print(f'Models params: \\nbucket: {model.getBucketLength()}, n_ht: {model.getNumHashTables()}')\n",
    "            \n",
    "#             df_pipe = model.transform(train_data)\n",
    "            looking_for = targets.VectorSpaceIDF.to_list()\n",
    "            print(f'Calculating LSH for bucket_length={bucket_length} and numHashTables = {n_hash_table}')\n",
    "            test = test_data.limit(limit)\n",
    "            targ = looking_for[:limit]\n",
    "            prediction = []\n",
    "            \n",
    "            \n",
    "            for key in tqdm.tqdm_notebook(targ):\n",
    "                neigh = find_neighbours(model, test, key, 6, 'hashes')\n",
    "                prediction.append(neigh)\n",
    "                \n",
    "            score_a, score_b, score_c, num_neighb = compare_lists(prediction, targets['ground_truth'])\n",
    "            print(f'Total score: {score_a}, {score_b}, {score_c}\\n {num_neighb[:20]} \\n{prediction[:20]}' )\n",
    "            results.append([bucket_length, n_hash_table, score_a, score_b, score_c, model])\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_bucket_length = [2, 10]\n",
    "grid_num_hash_tables = [1, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-03T22:46:59.899Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models params: \n",
      "bucket: 2.0, n_ht: 1\n",
      "Calculating LSH for bucket_length=2 and numHashTables = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-85-da23784d9854>:39: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for key in tqdm.tqdm_notebook(targ):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b7d66f48d641a582fdf79716784c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4065.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total score: 0.5685116851168511, 0.13554735547355473, 0.0002460024600246002\n",
      " [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1] \n",
      "[[39031875, 25927581, 24170185, 5197752, 42855696], [40060052, 41974762, 37520718, 8806793, 7173195], [39660659, 42355295, 4389803, 4691491, 23649134], [18540745, 37713410, 37554688, 34676896, 23477641], [26345440, 1391124, 39660659, 1070322, 41275665], [35598015, 6666031, 39623073, 6436766, 7301264], [34358699, 33727149, 45282768, 11685001, 21653709], [32213212, 8020706, 28803790, 2091401, 38035345], [15713824, 793232, 19320230, 44462767, 42020758], [45074881, 31818441, 29986591, 2288221, 27191542], [37713410, 37554688, 5407955, 17578576, 532775], [5695033, 29644147, 44160082, 6834341, 41545530], [22463569, 34117861, 23804717, 814202, 23511046], [12414795, 27797794, 12694296, 38694539, 15863971], [10208538, 25335202, 16891365, 39470732, 36896927], [29819343, 20258456, 38081807, 41394469, 43649088], [20437264, 18918174, 4957651, 34108603, 3203100], [1186559, 38691001, 33210949, 9620968, 6522117], [24496319, 7352850, 9142039, 40862473, 33178327], [670956, 19265548, 100256, 36218747, 44935619]]\n",
      "Models params: \n",
      "bucket: 2.0, n_ht: 10\n",
      "Calculating LSH for bucket_length=2 and numHashTables = 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67390f9ada9a48039b9f8ec28b51f1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4065.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total score: 0.6346863468634686, 0.15670356703567034, 0.0004920049200492004\n",
      " [0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1] \n",
      "[[4228483, 39031875, 25927581, 24170185, 5197752], [40060052, 41974762, 37520718, 8806793, 7173195], [1810923, 39753694, 39744593, 42600422, 39660659], [18540745, 37554688, 37713410, 34676896, 23477641], [26345440, 1391124, 39660659, 1070322, 41275665], [35598015, 6666031, 39623073, 43420312, 6436766], [34358699, 33727149, 45282768, 11685001, 21653709], [32213212, 6058351, 20597809, 8020706, 28803790], [15713824, 793232, 45282768, 24977031, 16385602], [43764033, 9626098, 45074881, 31818441, 36253888], [37713410, 37554688, 5407955, 17578576, 532775], [24662715, 5695033, 29644147, 44160082, 619728], [22463569, 40415475, 34117861, 23804717, 814202], [12414795, 39523332, 2117095, 20326192, 2476289], [10208538, 25335202, 16891365, 39470732, 36896927], [29819343, 20258456, 32546591, 2436237, 38081807], [20437264, 18918174, 4957651, 21722132, 34108603], [1186559, 38691001, 31818441, 33210949, 6522117], [24496319, 7352850, 9142039, 40862473, 33178327], [24626818, 670956, 19265548, 100256, 4639644]]\n",
      "Models params: \n",
      "bucket: 10.0, n_ht: 1\n",
      "Calculating LSH for bucket_length=10 and numHashTables = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecea4df6c4d4408facabfedba8002b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4065.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total score: 0.5724477244772448, 0.13628536285362855, 0.0002460024600246002\n",
      " [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1] \n",
      "[[39031875, 25927581, 24170185, 5197752, 42855696], [40060052, 41974762, 37520718, 8806793, 7173195], [39660659, 42355295, 4389803, 4691491, 23649134], [18540745, 37713410, 37554688, 34676896, 23477641], [26345440, 1391124, 39660659, 1070322, 41275665], [35598015, 6666031, 39623073, 6436766, 7301264], [34358699, 33727149, 45282768, 11685001, 21653709], [32213212, 8020706, 28803790, 2091401, 38035345], [15713824, 793232, 19320230, 44462767, 42020758], [45074881, 31818441, 29986591, 2288221, 27191542], [37713410, 37554688, 5407955, 17578576, 532775], [5695033, 29644147, 44160082, 6834341, 41545530], [22463569, 34117861, 23804717, 814202, 23511046], [12414795, 27797794, 12694296, 38694539, 15863971], [10208538, 25335202, 16891365, 39470732, 36896927], [29819343, 20258456, 38081807, 41394469, 43649088], [20437264, 18918174, 4957651, 34108603, 3203100], [1186559, 38691001, 33210949, 9620968, 6522117], [24496319, 7352850, 9142039, 40862473, 33178327], [670956, 19265548, 100256, 36218747, 44935619]]\n",
      "Models params: \n",
      "bucket: 10.0, n_ht: 10\n",
      "Calculating LSH for bucket_length=10 and numHashTables = 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f71db9350bd4941aec83a885e8cb7bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4065.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = grid_search_lsh(df_pipe, test_prepared, grid_bucket_length,\n",
    "                      grid_num_hash_tables, test_pd, limit=test_prepared.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T22:38:18.563241Z",
     "start_time": "2020-11-03T22:37:03.184Z"
    }
   },
   "source": [
    "## Wikidata experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_train = spark.read.parquet('train_test_data/wiki_train.pkt')\n",
    "wiki_test = spark.read.parquet('train_test_data/wiki_test.pkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizerWiki = Tokenizer(inputCol=\"Page\", outputCol=\"CleanTokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizerWiki, stopwordsremover, hashingTF, idf])\n",
    "\n",
    "pipelineWikiTrain = pipeline.fit(train)\n",
    "pipelineWikiTest = pipeline.fit(test)\n",
    "\n",
    "wiki_train_pipe = pipelineWikiTrain.transform(train)\n",
    "test_prepared_wiki = pipelineWikiTest.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pd_wiki = test_prepared_wiki.toPandas()\n",
    "looking_for_wiki = test_pd_wiki.VectorSpaceIDF.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_bucket_length = [2, 10]\n",
    "grid_num_hash_tables = [1, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained on Barcelona data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_barcelona_wiki = grid_search_lsh(df_pipe, test_prepared_wiki, grid_bucket_length,\n",
    "                      grid_num_hash_tables, test_pd_wiki, limit=test_prepared_wiki.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained on Wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_wiki = grid_search_lsh(wiki_train_pipe, test_prepared_wiki, grid_bucket_length,\n",
    "                      grid_num_hash_tables, test_pd_wiki, limit=test_prepared_wiki.count())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Mining Massive Datasets",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
