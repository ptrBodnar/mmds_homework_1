{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sq8U3BtmhtRx"
   },
   "source": [
    "\n",
    "#  **Running Pyspark in Colab**\n",
    "\n",
    "To run spark in Colab, we need to first install all the dependencies in Colab environment i.e. Apache Spark 2.3.2 with hadoop 2.7, Java 8 and Findspark to locate the spark in the system. The tools installation can be carried out inside the Jupyter Notebook of the Colab. One important note is that if you are new in Spark, it is better to avoid Spark 2.4.0 version since some people have already complained about its compatibility issue with python. \n",
    "Follow the steps to install the dependencies:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gdgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lh5NCoc8fsSO"
   },
   "outputs": [],
   "source": [
    "# !wget -q https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
    "# !tar xf spark-3.0.1-bin-hadoop2.7.tgz\n",
    "# !pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILheUROOhprv"
   },
   "source": [
    "Now that you installed Spark and Java in Colab, it is time to set the environment path which enables you to run Pyspark in your Colab environment. Set the location of Java and Spark by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T21:32:25.083933Z",
     "start_time": "2020-11-03T21:32:25.079854Z"
    },
    "id": "v1b8k_OVf2QF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"../spark-3.0.1-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwrqMk3HiMiE"
   },
   "source": [
    "Run a local spark session to test your installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T21:32:25.677149Z",
     "start_time": "2020-11-03T21:32:25.671448Z"
    },
    "id": "9_Uz1NL4gHFx"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T18:42:50.192864Z",
     "start_time": "2020-11-03T18:42:45.809229Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "vERDoZz3TXx2",
    "outputId": "da327a51-5b0b-4f7b-818d-dc731d964ef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-03 20:42:45--  http://data.insideairbnb.com/spain/catalonia/barcelona/2020-09-12/visualisations/listings.csv\n",
      "Resolving data.insideairbnb.com (data.insideairbnb.com)... 52.216.77.19\n",
      "Connecting to data.insideairbnb.com (data.insideairbnb.com)|52.216.77.19|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3137507 (3,0M) [application/csv]\n",
      "Saving to: ‘listings.csv’\n",
      "\n",
      "listings.csv        100%[===================>]   2,99M   836KB/s    in 3,7s    \n",
      "\n",
      "2020-11-03 20:42:50 (836 KB/s) - ‘listings.csv’ saved [3137507/3137507]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ! wget http://data.insideairbnb.com/the-netherlands/north-holland/amsterdam/2020-08-18/data/listings.csv\n",
    "# ! wget http://data.insideairbnb.com/the-netherlands/north-holland/amsterdam/2020-08-18/data/reviews.csv\n",
    "# ! wget http://data.insideairbnb.com/spain/catalonia/barcelona/2020-09-12/visualisations/listings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T21:32:27.584949Z",
     "start_time": "2020-11-03T21:32:27.580417Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T22:35:41.491611Z",
     "start_time": "2020-11-03T22:35:41.062126Z"
    },
    "id": "zxwi2KauVTTD"
   },
   "outputs": [],
   "source": [
    "listings = spark.read.csv('listings.csv',inferSchema=True, header =True)\n",
    "\n",
    "train = spark.read.parquet('train_test_data/train.pkt')\n",
    "test = spark.read.parquet('train_test_data/test.pkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T22:35:42.079587Z",
     "start_time": "2020-11-03T22:35:42.030483Z"
    },
    "id": "13ZurAGz4nwv"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"name\", outputCol=\"CleanTokens\")\n",
    "stopwordsremover = StopWordsRemover(inputCol=\"CleanTokens\", outputCol=\"CleanTokensStopRemoved\")\n",
    "hashingTF = HashingTF(inputCol=\"CleanTokensStopRemoved\", outputCol=\"VectorSpace\", numFeatures=50)\n",
    "idf = IDF(inputCol=\"VectorSpace\", outputCol=\"VectorSpaceIDF\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T22:35:42.608866Z",
     "start_time": "2020-11-03T22:35:42.225291Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, stopwordsremover, hashingTF, idf])\n",
    "pipelineModel = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T22:35:42.672472Z",
     "start_time": "2020-11-03T22:35:42.611413Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_pipe = pipelineModel.transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T22:35:44.222733Z",
     "start_time": "2020-11-03T22:35:43.959336Z"
    }
   },
   "outputs": [],
   "source": [
    "pipelineModelTest = pipeline.fit(test)\n",
    "test_prepared = pipelineModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T22:35:45.479648Z",
     "start_time": "2020-11-03T22:35:44.248893Z"
    }
   },
   "outputs": [],
   "source": [
    "test_pd = test_prepared.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T22:35:45.484800Z",
     "start_time": "2020-11-03T22:35:45.481439Z"
    }
   },
   "outputs": [],
   "source": [
    "looking_for = test_pd.VectorSpaceIDF.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T21:32:34.167671Z",
     "start_time": "2020-11-03T21:32:34.161390Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_neighbours(data, value, number, colName):\n",
    "    result = model.approxNearestNeighbors(data, value, number, distCol=colName)\n",
    "  \n",
    "    return result.select(\"id\").toPandas()['id'][1:].to_list()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's slow anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T19:56:59.912909Z",
     "start_time": "2020-11-03T19:56:59.906550Z"
    }
   },
   "outputs": [],
   "source": [
    "def compare_lists(a,b):\n",
    "    results = []\n",
    "    for i, _ in enumerate(a):\n",
    "        intersection = set(a[i]).intersection(b[i])\n",
    "        results.append(len(intersection))\n",
    "        \n",
    "    return np.mean(results), np.sum(np.array(results) > 1)/len(b), np.sum(np.array(results) == 5)/len(b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T19:57:00.174893Z",
     "start_time": "2020-11-03T19:57:00.137465Z"
    }
   },
   "outputs": [],
   "source": [
    "score_a, score_b, score_c = compare_lists(out, test_pd['ground_truth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T19:56:53.802489Z",
     "start_time": "2020-11-03T19:56:53.797218Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.7424354243542435, 0.5682656826568265, 0.07060270602706027)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_a, score_b, score_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T19:10:00.787612Z",
     "start_time": "2020-11-03T19:10:00.777877Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177           Beautiful Apartment Sagrada Familia\n",
       "206           Gran Vía Apartment Catalunya square\n",
       "285            Beautiful Double Room With Balcony\n",
       "327                                Cozy Apartment\n",
       "331    Beautiful Double Room with private terrace\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pd.loc[test_pd.id.isin(out[5])].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T22:46:44.992587Z",
     "start_time": "2020-11-03T22:46:44.977755Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_bucket_length = [2, 10]\n",
    "grid_num_hash_tables = [1, 10]\n",
    "\n",
    "def find_neighbours(model_, data, value, number, colName):\n",
    "    result = model_.approxNearestNeighbors(data, value, number, distCol=colName)\n",
    "    \n",
    "    return result.select(\"id\").toPandas()['id'][1:].to_list()\n",
    "\n",
    "def compare_lists(a,b):\n",
    "    results = []\n",
    "    for i, _ in enumerate(a):\n",
    "        intersection = set(a[i]).intersection(b[i])\n",
    "        results.append(len(intersection))\n",
    "        \n",
    "#     return np.mean(results), results\n",
    "    return np.mean(results), np.sum(np.array(results) > 1)/len(b), np.sum(np.array(results) == 5)/len(b), results\n",
    "\n",
    "\n",
    "def grid_search_lsh(train_data, test_data, grid_bucket_length, grid_num_hash_tables, targets, limit=300):\n",
    "    results = []  \n",
    "    for bucket_length in grid_bucket_length:\n",
    "        for n_hash_table in grid_num_hash_tables: \n",
    "            brp = BucketedRandomProjectionLSH(inputCol=\"VectorSpaceIDF\", \n",
    "                                              outputCol=\"hashes\", \n",
    "                                              bucketLength=bucket_length,\n",
    "                                              numHashTables=n_hash_table)\n",
    "            # fit train\n",
    "            model = brp.fit(train_data)\n",
    "            print(f'Models params: \\nbucket: {model.getBucketLength()}, n_ht: {model.getNumHashTables()}')\n",
    "            \n",
    "#             df_pipe = model.transform(train_data)\n",
    "            looking_for = targets.VectorSpaceIDF.to_list()\n",
    "            print(f'Calculating LSH for bucket_length={bucket_length} and numHashTables = {n_hash_table}')\n",
    "            test = test_data.limit(limit)\n",
    "            targ = looking_for[:limit]\n",
    "            prediction = []\n",
    "            \n",
    "            \n",
    "            for key in tqdm.tqdm_notebook(targ):\n",
    "                neigh = find_neighbours(model, test, key, 6, 'hashes')\n",
    "                prediction.append(neigh)\n",
    "                \n",
    "            score_a, score_b, score_c, num_neighb = compare_lists(prediction, targets['ground_truth'])\n",
    "            print(f'Total score: {score_a}, {score_b}, {score_c}\\n {num_neighb[:20]} \\n{prediction[:20]}' )\n",
    "            results.append([bucket_length, n_hash_table, score_a, score_b, score_c, model])\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-03T22:46:59.899Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models params: \n",
      "bucket: 2.0, n_ht: 1\n",
      "Calculating LSH for bucket_length=2 and numHashTables = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-85-da23784d9854>:39: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for key in tqdm.tqdm_notebook(targ):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b7d66f48d641a582fdf79716784c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4065.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = grid_search_lsh(df_pipe, test_prepared, grid_bucket_length,\n",
    "                      grid_num_hash_tables, test_pd, limit=test_prepared.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T22:38:18.563241Z",
     "start_time": "2020-11-03T22:37:03.184Z"
    }
   },
   "outputs": [],
   "source": [
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Mining Massive Datasets",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
